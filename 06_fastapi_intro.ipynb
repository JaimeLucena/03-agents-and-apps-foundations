{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5f08b7",
   "metadata": {},
   "source": [
    "# 06 ‚Äî Intro to FastAPI\n",
    "\n",
    "In this notebook you‚Äôll learn the core ideas of **FastAPI**, one of the most popular modern Python frameworks for building web APIs.\n",
    "\n",
    "We‚Äôll explain what FastAPI is, why it‚Äôs used so much in AI and backend projects, and finish with a simple, fully working example you can run locally in a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf84f65",
   "metadata": {},
   "source": [
    "## üß† What is FastAPI?\n",
    "\n",
    "FastAPI is a **modern Python framework for building web APIs** ‚Äî that is, programs that expose endpoints such as `/chat`, `/predict`, or `/healthz` that other software (or a frontend app) can call.\n",
    "\n",
    "It‚Äôs built on top of **Starlette** (for performance) and **Pydantic** (for data validation). It‚Äôs known for being **fast**, **typed**, **async**, and **developer-friendly**.\n",
    "\n",
    "**Typical use cases:**\n",
    "- Exposing AI models as APIs (LLMs, RAG pipelines, agents...)\n",
    "- Creating chat endpoints for web or mobile apps\n",
    "- Serving predictions, summaries, or analytics\n",
    "- Health checks, metrics, and monitoring endpoints\n",
    "\n",
    "üëâ In short, FastAPI lets you wrap your Python logic (LangChain, CrewAI, etc.) into a simple web service that can be consumed by anything ‚Äî a frontend, another backend, or even yourself with `curl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7833a",
   "metadata": {},
   "source": [
    "## ‚ö° Why FastAPI?\n",
    "\n",
    "FastAPI became the go-to choice for Python APIs because:\n",
    "\n",
    "- ‚ö° **Fast:** built on asyncio + Starlette ‚Üí handles many requests efficiently.\n",
    "- üß© **Typed:** uses Python type hints to auto-generate validation and docs.\n",
    "- üìò **Automatic docs:** built-in Swagger UI at `/docs`.\n",
    "- üí° **Productive:** minimal boilerplate, clean syntax.\n",
    "- ü§ù **Compatible:** works perfectly with Pydantic, LangChain, SQLAlchemy, etc.\n",
    "\n",
    "It‚Äôs ideal for **both small prototypes** and **production-grade applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57629274",
   "metadata": {},
   "source": [
    "## üß© Basic idea\n",
    "\n",
    "A FastAPI app is simply a Python file that:\n",
    "\n",
    "1. Creates a `FastAPI()` instance.\n",
    "2. Defines endpoints with decorators like `@app.get()` or `@app.post()`.\n",
    "3. Returns JSON responses.\n",
    "\n",
    "Let‚Äôs build a **minimal working API** that uses a real LLM (OpenAI or Groq) through LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py ‚Äî A minimal FastAPI app calling a real LLM\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# ---- Load environment (.env) ----\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "app = FastAPI(title=\"FastAPI + LLM Demo\")\n",
    "\n",
    "# ---- Pydantic schema for /chat ----\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    thread_id: Optional[str] = None  # optional future use (memory)\n",
    "\n",
    "# ---- Lazy model factory (OpenAI by default, optional Groq) ----\n",
    "_chat_model = None\n",
    "\n",
    "def get_model():\n",
    "    global _chat_model\n",
    "    if _chat_model is not None:\n",
    "        return _chat_model\n",
    "\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"openai\").lower()\n",
    "\n",
    "    if provider == \"groq\":\n",
    "        from langchain_groq import ChatGroq\n",
    "        model_name = os.getenv(\"GROQ_MODEL\", \"llama-3.1-70b-versatile\")\n",
    "        _chat_model = ChatGroq(model=model_name)\n",
    "    else:\n",
    "        model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "        _chat_model = ChatOpenAI(model=model_name)\n",
    "\n",
    "    return _chat_model\n",
    "\n",
    "\n",
    "# ---- Simple health endpoint ----\n",
    "@app.get(\"/hello\")\n",
    "def say_hello(name: str = \"World\"):\n",
    "    return {\"message\": f\"Hello, {name}!\"}\n",
    "\n",
    "\n",
    "# ---- POST /chat: calls the LLM ----\n",
    "@app.post(\"/chat\")\n",
    "async def chat(req: ChatRequest):\n",
    "    \"\"\"\n",
    "    Minimal chat endpoint:\n",
    "    - Builds a short system instruction.\n",
    "    - Sends user's message to the LLM.\n",
    "    - Returns the model's text content.\n",
    "    \"\"\"\n",
    "    model = get_model()\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful and concise assistant.\"),\n",
    "        HumanMessage(content=req.message),\n",
    "    ]\n",
    "    try:\n",
    "        resp = await model.ainvoke(messages)\n",
    "        return {\"reply\": resp.content}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"LLM error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0239c",
   "metadata": {},
   "source": [
    "## üß™ Running locally\n",
    "\n",
    "Start your FastAPI app with **Uvicorn**, the ASGI server that powers it.\n",
    "\n",
    "In your terminal (from this repo‚Äôs root):\n",
    "\n",
    "```bash\n",
    "uv run uvicorn main:app --reload --port 8000\n",
    "```\n",
    "\n",
    "or, if you don‚Äôt use `uv`:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload --port 8000\n",
    "```\n",
    "\n",
    "**Then open your browser at:**\n",
    "\n",
    "- üëâ http://localhost:8000/docs ‚Äî interactive Swagger UI\n",
    "- üëâ http://localhost:8000/hello ‚Äî test GET request\n",
    "- üëâ POST /chat ‚Äî test sending a message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a073fdd",
   "metadata": {},
   "source": [
    "## üß± Example requests\n",
    "\n",
    "**Using your browser:**\n",
    "\n",
    "```\n",
    "http://localhost:8000/hello?name=Jaime\n",
    "```\n",
    "\n",
    "‚û°Ô∏è Response:\n",
    "```json\n",
    "{ \"message\": \"Hello, Jaime!\" }\n",
    "```\n",
    "\n",
    "**Using curl:**\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/chat \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"message\": \"Hi there!\"}'\n",
    "```\n",
    "\n",
    "‚û°Ô∏è Response:\n",
    "```json\n",
    "{ \"reply\": \"Hello! How can I assist you today?\" }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ddbcc3",
   "metadata": {},
   "source": [
    "## üß© Adding Pydantic models (for clarity)\n",
    "\n",
    "In real apps, you‚Äôll use **Pydantic models** to define your input and output data. This gives automatic validation and documentation.\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(title=\"Chat API\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    reply: str\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "def chat(req: ChatRequest):\n",
    "    reply = f\"Echo: {req.message}\"\n",
    "    return ChatResponse(reply=reply)\n",
    "```\n",
    "\n",
    "Now `/docs` will automatically show request and response schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e68d73",
   "metadata": {},
   "source": [
    "## üß© How this connects with AI apps\n",
    "\n",
    "This FastAPI layer acts as the **gateway** to your AI logic. For example:\n",
    "\n",
    "- `@app.post('/chat')` ‚Üí calls your LangGraph or CrewAI agent.\n",
    "- `@app.post('/rag')` ‚Üí triggers a retrieval pipeline.\n",
    "- `@app.get('/healthz')` ‚Üí exposes a monitoring endpoint.\n",
    "\n",
    "Keep this layer **small**, **stateless**, and **easy to test** ‚Äî the heavy logic lives in your LangChain or LangGraph modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb67e2",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- **FastAPI** is a high-performance web framework for APIs.\n",
    "- Each endpoint is a simple Python function with `@app.get()` or `@app.post()`.\n",
    "- Requests and responses are automatically parsed and validated.\n",
    "- Built-in docs make exploration effortless.\n",
    "- Perfect for connecting LangChain / LangGraph / CrewAI logic into real-world applications.\n",
    "\n",
    "üöÄ Next, you‚Äôll connect this backend to **Streamlit** to build a complete interactive AI app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
